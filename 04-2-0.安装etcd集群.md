[TOC]

## 准备工作
三节点均执行
1. [01.系统初始化.md](https://github.com/wanilyer/kubeadm/blob/master/01.%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96.md)
2. [02.安装docker和kubeadm.md](https://github.com/wanilyer/kubeadm/blob/master/02.%E5%AE%89%E8%A3%85docker%E5%92%8Ckubeadm.md)


## 集群安装

### 准备etcd host(10-export-etcd-hosts.sh)

```
#!/bin/bash

export HOST0=10.142.99.209
export HOST1=10.142.97.172
export HOST2=10.142.97.136

export HOSTS=($HOST0 $HOST1 $HOST2)
```

### kubelet配置
```
#!/bin/bash

source ./10-export-etcd-hosts.sh

# 配置kubelet启动参数: KUBELET_EXTRA_ARGS
# 参考: https://kubernetes.io/docs/setup/independent/kubelet-integration/#the-kubelet-drop-in-file-for-systemd
cat << EOF > kubelet 
KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1
EOF

# 配置 kubelet cgroup driver
# EOF上的单引号表示输出到文件时，不替换其中的变量，因为KUBELET_EXTRA_ARGS变量在kubelet启动时会自己替换
cat << 'EOF' > 20-etcd-service-manager.conf
[Service]
ExecStart=
#  Replace "systemd" with the cgroup driver of your container runtime. The default value in the kubelet is "cgroupfs".
ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd $KUBELET_EXTRA_ARGS
Restart=always
EOF

# 远程机器上执行脚本
cat << EOF > config-systemd.sh
#!/bin/bash
mkdir -p /etc/systemd/system/kubelet.service.d/
mv 20-etcd-service-manager.conf /etc/systemd/system/kubelet.service.d/

mv kubelet /etc/sysconfig/

systemctl daemon-reload
systemctl restart kubelet
EOF

for HOST in ${HOSTS[@]}
  do
    echo ">>> ${HOST}"
    scp -r kubelet root@${HOST}:
    scp -r 20-etcd-service-manager.conf root@${HOST}:
    scp -r config-systemd.sh root@${HOST}:
    ssh root@${HOST} \
      "chmod +x config-systemd.sh; sh ./config-systemd.sh; exit"
  done
```

### kubeadm配置
```
#!/bin/bash

source ./10-export-etcd-hosts.sh

# Create temp directories to store files that will end up on other hosts.
mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/

ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2})
NAMES=("infra0" "infra1" "infra2")

for i in "${!ETCDHOSTS[@]}"; do
HOST=${ETCDHOSTS[$i]}
NAME=${NAMES[$i]}
cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml
apiVersion: "kubeadm.k8s.io/v1beta2"
kind: ClusterConfiguration
kubernetesVersion: v1.16.2
etcd:
    local:
        serverCertSANs:
        - "${HOST}"
        peerCertSANs:
        - "${HOST}"
        extraArgs:
            initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380
            initial-cluster-state: new
            name: ${NAME}
            listen-peer-urls: https://${HOST}:2380
            listen-client-urls: https://${HOST}:2379
            advertise-client-urls: https://${HOST}:2379
            initial-advertise-peer-urls: https://${HOST}:2380
imageRepository: registry.aliyuncs.com/google_containers # 修改镜像仓库地址，默认为k8s.gcr.io 会拉取失败
EOF
done
```
+ kubernetesVersion： 版本设置

+ imageRepository：镜像仓库

+ serverCertSANs和peerCertSANs为官方文档配置，但在etcd集群出现故障时，有可能遇到遇到问题。可参考：https://github.com/kubernetes/kubernetes/issues/72102

  

### 生成证书
```
#!/bin/bash

source ./10-export-etcd-hosts.sh

kubeadm init phase certs etcd-ca

kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST2}/
# cleanup non-reusable certificates
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST1}/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
# No need to move the certs because they are for HOST0

# clean up certs that should not be copied off this host
find /tmp/${HOST2} -name ca.key -type f -delete
find /tmp/${HOST1} -name ca.key -type f -delete
```



### 拷贝证书到其它节点

```
#!/bin/bash

source ./10-export-etcd-hosts.sh

export HOSTS=(${HOST1} ${HOST2})

for HOST in ${HOSTS[@]}
  do
    echo ">>> ${HOST}"
    scp -r /tmp/${HOST}/* root@${HOST}:
    ssh root@${HOST} \
      "mkdir -p /etc/kubernetes/;rm -rf /etc/kubernetes/pki/; mv -f pki /etc/kubernetes/;exit"
  done
```



### 启动集群

```
#!/bin/bash

source ./10-export-etcd-hosts.sh

kubeadm init phase kubelet-start --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml
ssh root@${HOST1} "kubeadm init phase kubelet-start --config=/root/kubeadmcfg.yaml; kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml"
ssh root@${HOST2} "kubeadm init phase kubelet-start --config=/root/kubeadmcfg.yaml; kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml"
```



### 验证

```
docker run --rm -it --net host -v /etc/kubernetes:/etc/kubernetes registry.aliyuncs.com/google_containers/etcd:3.3.10 etcdctl --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --ca-file /etc/kubernetes/pki/etcd/ca.crt --endpoints https://10.142.99.209:2379 cluster-health
```
```
member 358f64d1d78fa86a is healthy: got healthy result from https://10.142.97.172:2379
member 9835bed99549a546 is healthy: got healthy result from https://10.142.97.136:2379
member d5622f19a6d93a02 is healthy: got healthy result from https://10.142.99.209:2379
cluster is healthy
```



### 问题集锦


1. kubelet服务不正常，一般都是`cgroup driver`不一致导致的问题
```
## 问题原因
## failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: "systemd" is different from docker cgroup driver: "cgroupfs"
## 修改docker的cgroup driver和kubelet一致
```

2. 镜像拉取失败
```
## pause镜像拉取失败，修改kubelet启动参数
## 参考：https://kubernetes.io/docs/setup/independent/kubelet-integration/#the-kubelet-drop-in-file-for-systemd

cat >/etc/sysconfig/kubelet<<EOF
KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1
EOF

## etcd镜像拉取失败，修改kubeadm配置参数
imageRepository: registry.aliyuncs.com/google_containers
```

## 参考
+ https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/
+ https://kubernetes.io/docs/setup/independent/kubelet-integration/#the-kubelet-drop-in-file-for-systemd
+ https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/