[TOC]

## 采用kubeadm安装etcd集群

### 1. 准备工作
三etcd节点均执行以下操作
1. [01.系统初始化.md](https://github.com/wanilyer/kubeadm/blob/master/01.%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96.md)

2. [02.安装docker和kubeadm.md](https://github.com/wanilyer/kubeadm/blob/master/02.%E5%AE%89%E8%A3%85docker%E5%92%8Ckubeadm.md)
+ 注意：docker安装使用以下脚本，替换cgroupfs为systemd，当然使用cgroupfs也是可以的，只要保证kubelet和docker使用一致，只是官方建议使用systemd
```
#!/bin/bash

yum install -y docker-ce-18.09.9-3.el7
systemctl enable docker
systemctl start docker

cat > /etc/docker/daemon.json <<EOF
{
  "registry-mirrors": ["https://xxx.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

systemctl restart docker
```
3. 准备etcd host(10-export-etcd-hosts.sh)

```
#!/bin/bash

export HOST0=10.142.99.209
export HOST1=10.142.97.172
export HOST2=10.142.97.136

export HOSTS=($HOST0 $HOST1 $HOST2)
```

### 2. 配置kubelet
```
#!/bin/bash

source ./10-export-etcd-hosts.sh

# 配置kubelet启动参数: KUBELET_EXTRA_ARGS
cat << EOF > kubelet 
KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1
EOF

# 配置 kubelet cgroup driver
# EOF上的单引号表示输出到文件时，不替换其中的变量
# kubelet启动时会去/etc/sysconfig/kubelet文件中查找KUBELET_EXTRA_ARGS变量的值
cat << 'EOF' > 20-etcd-service-manager.conf
[Service]
ExecStart=
#  Replace "systemd" with the cgroup driver of your container runtime. The default value in the kubelet is "cgroupfs".
ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd $KUBELET_EXTRA_ARGS
Restart=always
EOF

# 远程机器上执行脚本
cat << EOF > config-systemd.sh
#!/bin/bash
mkdir -p /etc/systemd/system/kubelet.service.d/
mv 20-etcd-service-manager.conf /etc/systemd/system/kubelet.service.d/

mv kubelet /etc/sysconfig/

systemctl daemon-reload
systemctl restart kubelet
EOF

for HOST in ${HOSTS[@]}
  do
    echo ">>> ${HOST}"
    scp -r kubelet root@${HOST}:
    scp -r 20-etcd-service-manager.conf root@${HOST}:
    scp -r config-systemd.sh root@${HOST}:
    ssh root@${HOST} \
      "chmod +x config-systemd.sh; sh ./config-systemd.sh; exit"
  done
```
+ `pause镜像问题`
```
## --pod-infra-container-image: 修改默认镜像拉取地址, 不设置此参数的话，默认会从k8s.gcr.io拉取pause镜像, 拉取失败后无法启动etcd容器
## 关于kubelet启动参数设置，如：KUBELET_EXTRA_ARGS，请参考: https://kubernetes.io/docs/setup/independent/kubelet-integration/#the-kubelet-drop-in-file-for-systemd
```

+ `cgroup-driver, docker和kubelet需要保证一致，我们这里采用官方建议的systemd`


### 3. kubeadm配置
```
#!/bin/bash

source ./10-export-etcd-hosts.sh

# Create temp directories to store files that will end up on other hosts.
mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/

ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2})
NAMES=("infra0" "infra1" "infra2")

for i in "${!ETCDHOSTS[@]}"; do
HOST=${ETCDHOSTS[$i]}
NAME=${NAMES[$i]}
cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml
apiVersion: "kubeadm.k8s.io/v1beta2"
kind: ClusterConfiguration
kubernetesVersion: v1.16.2
etcd:
    local:
        serverCertSANs:
        - "${HOST}"
        peerCertSANs:
        - "${HOST}"
        extraArgs:
            initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380
            initial-cluster-state: new
            name: ${NAME}
            listen-peer-urls: https://${HOST}:2380
            listen-client-urls: https://${HOST}:2379
            advertise-client-urls: https://${HOST}:2379
            initial-advertise-peer-urls: https://${HOST}:2380
imageRepository: registry.aliyuncs.com/google_containers # 修改镜像仓库地址，默认为k8s.gcr.io 会拉取失败
EOF
done
```
+ kubernetesVersion： 版本设置

+ imageRepository：镜像仓库

  

### 4. 生成证书
```
#!/bin/bash

source ./10-export-etcd-hosts.sh

kubeadm init phase certs etcd-ca

kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST2}/
# cleanup non-reusable certificates
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST1}/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
# No need to move the certs because they are for HOST0

# clean up certs that should not be copied off this host
find /tmp/${HOST2} -name ca.key -type f -delete
find /tmp/${HOST1} -name ca.key -type f -delete
```



### 5. 拷贝证书到其它两节点

```
#!/bin/bash

source ./10-export-etcd-hosts.sh

export HOSTS=(${HOST1} ${HOST2})

for HOST in ${HOSTS[@]}
  do
    echo ">>> ${HOST}"
    scp -r /tmp/${HOST}/* root@${HOST}:
    ssh root@${HOST} \
      "mkdir -p /etc/kubernetes/;rm -rf /etc/kubernetes/pki/; mv -f pki /etc/kubernetes/;exit"
  done
```



### 6. 启动集群

```
#!/bin/bash

source ./10-export-etcd-hosts.sh

kubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml
ssh root@${HOST1} "kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml"
ssh root@${HOST2} "kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml"
```



### 7. 验证

```
docker run --rm -it --net host -v /etc/kubernetes:/etc/kubernetes registry.aliyuncs.com/google_containers/etcd:3.3.10 etcdctl --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --ca-file /etc/kubernetes/pki/etcd/ca.crt --endpoints https://10.142.99.209:2379 cluster-health
```
```
member 358f64d1d78fa86a is healthy: got healthy result from https://10.142.97.172:2379
member 9835bed99549a546 is healthy: got healthy result from https://10.142.97.136:2379
member d5622f19a6d93a02 is healthy: got healthy result from https://10.142.99.209:2379
cluster is healthy
```



### 8. 问题集锦


1. kubelet服务部正常，一般都是`cgroup driver`不一致导致的问题
```
## 问题原因
## failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: "systemd" is different from docker cgroup driver: "cgroupfs"
## 修改docker的cgroup driver和kubelet一致
```

2. 镜像拉取失败
```
## pause镜像拉取失败，修改kubelet启动参数
## 参考：https://kubernetes.io/docs/setup/independent/kubelet-integration/#the-kubelet-drop-in-file-for-systemd

cat >/etc/sysconfig/kubelet<<EOF
KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1
EOF

## etcd镜像拉取失败，修改kubeadm配置参数
imageRepository: registry.aliyuncs.com/google_containers
```

## 参考
+ https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/
+ https://kubernetes.io/docs/setup/independent/kubelet-integration/#the-kubelet-drop-in-file-for-systemd
+ https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/